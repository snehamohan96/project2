# -*- coding: utf-8 -*-
"""carpriceprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G7wwDmTyEtNgM-L6az7zvSFt7Q7fPV6t

**PROBLEM STATEMENT**

Aim to develop a machine learning model that accurately estimates the price of different car models based on a wide range of attributes and features.

**IMPORT LIBRARIES AND LOAD DATASET**
"""

# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.ensemble import RandomForestRegressor

df = pd.read_csv('/content/CarPrice_prediction1.csv') # load the data

"""**BASIC DATA INFORMATION AND DATA CLEANING**"""

df.head() # display first 5 rows of dataset

df.tail() # dispaly last 5 rows of dataframe

df.shape # display total number of rows and columns

df.describe() # descriptive statistics of numerical columns

df.info()#display column information such as column name,data type, non_null

df.dtypes # display the column and its data type

df.isnull().sum()# count number of missing values in each column

"""when checking null values in the dataset , it is observed that there are null values in the columns 'carwidth' and 'carheight'.

Number of null values in 'carwidth':2

Number of null values in 'carheight':3
"""

# Fixing null values

df['carwidth'].fillna(df['carwidth'].mean(), inplace=True)  # fill null values in the column 'carwidth' with mean
df['carheight'].fillna(df['carheight'].mean(), inplace=True) # fill null values in the column 'carheight' with mean

"""Here, I filled null values in these columns with mean values of respective columns."""

df.isnull().sum() # checking null values

"""Now there is no null values in the dataset"""

df.duplicated().sum()# count number of duplicate rows in the dataframe

"""Here, it is observed that there is no duplicate values"""

df.nunique() # count number of unique values in each column

# check unique values in each categorical column
categorical_columns = ['fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype',
    'cylindernumber','fuelsystem']

for col in categorical_columns:

    print(f"Category in {col} is : {df[col].unique()}")

"""**DATA VISUALIZATION**"""

# histogram to show the distribution of price
plt.figure(figsize=(8, 6))
sns.histplot(data=df['price'], bins=20, kde=True)
plt.title('Distribution of Price')
plt.show()

"""This graph shows the frequency count of different price ranges.Here, we can see a skewed distribution, that is majority is lower price."""

#Define the list of categorical columns to analyze
categorical_columns = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel',
                       'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem']

# Create subplots
fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12, 9))
axes = axes.ravel()  # Flatten the 2D array of axes

# Loop through each categorical column
for i, column in enumerate(categorical_columns):
    sns.countplot(x=df[column], data=df, palette='bright', ax=axes[i], saturation=0.95)
    for container in axes[i].containers:
        axes[i].bar_label(container, color='black', size=10)
    axes[i].set_title(f'Count Plot of {column.capitalize()}')
    axes[i].set_xlabel(column.capitalize())
    axes[i].set_ylabel('Count')

# Adjust layout and show plots
plt.tight_layout()
plt.show()

"""Observations


*   In 'fueltype' ,majority cars are using gas than diesel
*   In the case of Aspiration, most of cars have standard aspiration than turbocharging.

*   Most of car have four doors than two.
*   Majority of the car have sedan bodytype and very less have convertible.

*   Majority of car wheels are fwd and manority is 4wd.
*   Engine location of most of the car are front than rear.

*   In the case of 'Enginetype' majority have ohc and very less have dohcv.
*   Majority of car have cylinder number 4 and manority is 3 and 12.

*   Majority of car have mpfi fuel system and manority are mfi and spfi.













"""

# BAR PLOT TO SHOW TOP 20 CAR MODEL BASED ON AVERAGE PRICE

# Calculate average price for each car model
avg_prices_by_car = df.groupby('CarName')['price'].mean().sort_values(ascending=False)

# Take the top 20 car models
top_20_cars = avg_prices_by_car.head(20)

# Plotting
plt.figure(figsize=(12, 8))
sns.barplot(x=top_20_cars.values, y=top_20_cars.index, palette="viridis")# plotting barplot using seaborn
plt.xlabel('Average Price')
plt.ylabel('Car Model')
plt.title('Top 20 Car Models Based on Average Price')
plt.show()

"""Observations:


*  From this graph it is observed that 'buick regalsportcoupe(turbo)'  carmodel is in first position because its average price is higher than other car models.

*  volvo 246 is last position because its average price lower than other car models.


*   bmw x5 is in second position and buick century special is in third position.
*   car models like porcshce cayenne,porcshce panamera and jaguar xj are in almost same position because its average price is in same range.





"""

# Box plot of Categorical Feature vs. Price

plt.figure(figsize=(12, 8)) # setting figure size
for feature in categorical_columns:  # looping through categorical column
    plt.subplot(3, 3, categorical_columns.index(feature) + 1)
    sns.boxplot(data=df, x=feature, y='price') # seaborn is used to plot boxplot
    plt.title(f'{feature} vs. Price')# title of the graph
plt.tight_layout()
plt.show()

"""This box plot shows the relation of price with different car attributes.

Observations:


*   Diesel cars have a lower price compared to gasoline cars.And also gasoline cars have wider range of price.
*   Turbocharged cars have higher median price than standard aspiration cars.


*   Four door cars have higher price than two door cars.
*   Hardtop and convertible cars have higherprice and also sedan and hatchbacks have wider range of price.


*   RWD cars have higher price than FWD cars.
*   Rear-engine  cars have higher price than front-engine.

*   Rotary engines have higher price
*   cars with MPFI fuel system have higher price.








"""

# Heat map for Correlation Analysis

numerical_columns=['carlength','carwidth','carheight','curbweight','enginesize','boreratio','stroke','horsepower',
                   'compressionratio','peakrpm','citympg','highwaympg','price']
correlation_matrix = df[numerical_columns].corr()# compute correaltion matrix
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')# plotting heatmap using seaborn
plt.title('Correlation Heatmap')
plt.show()

"""Observations from heatmap:

*  The target variable 'price'  is highly correlated with curbweight, enginesize,horsepower and carwidth.
*   price is negatively correlated with 'citympg'and highwaympg.

*   Independent variables like enginesize and curbweight ,enginesize and horsepower are highly correlated with eachother.
*   Also independent variables like curbweight and carlength have a high correlation with eachother.

**DATA PREPROCESSING**
"""

#Extract brand and model from CarName
df['brand'] = df['CarName'].apply(lambda x: x.split(' ')[0])
df['model'] = df['CarName'].apply(lambda x: ' '.join(x.split(' ')[1:]))

# Define categorical and numerical columns
categorical_columns = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel',
                       'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'brand', 'model']
numerical_columns = ['wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight',
                     'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower',
                     'peakrpm', 'citympg', 'highwaympg']

# Data encoding using label encoding

label_encoder = LabelEncoder() # initialize label encoder
for column in categorical_columns: # looping through categorical columns
    df[column] = label_encoder.fit_transform(df[column]) # encode its values

df.head() # checking the encoding

#Feature engineering

df['power_to_weight_ratio'] = df['horsepower'] / df['curbweight'] # created a new column in df 'power_to_weight_ratio'for better analysis

for column in numerical_columns:
    df[f'{column}_squared'] = df[column] ** 2     # create  new feature by squaring the  values in numerical column to capturing non linear relationship between features for better performance

df['log_enginesize'] = np.log(df['enginesize'] + 1) # calculated natural logarithm of 'enginesize' column for better performance

# Feature scaling
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns]) #apply feature scaling to numerical columns to standardize the features to same scale

df.head() # checking scaling

"""**SPLITTING DATA INTO TRAIN AND TEST**"""

# Splitting the data
X = df.drop(['price', 'CarName'], axis=1)  # drop columns 'price' and 'carname' and assign  all other features to x.
y = df['price']     # assign 'price' column to y
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# splitting the dataset into random  train and test using train_test_split() function.

"""**MODEL SELECTION AND TRAIN THE MODEL**

**MODEL TRAINING USING LINEAR REGRESSION**
"""

# Model training using linear Regression
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

"""**MODEL EVALUATION OF LINEAR REGRESSION MODEL**"""

# Evaluate the model
mse = mean_squared_error(y_test, y_pred) # calculate mean squared error between predicted target value and actual target value
r2_square = r2_score(y_test,y_pred)    # calcualte R squared value
print(f" R-squared: {r2_square}")
print(f'Mean Squared Error: {mse}')

"""**MODEL TRAINING USING RANDOM FOREST**"""

# Model training using Random forest

random_forest_regressor = RandomForestRegressor(n_estimators=100, random_state=42) #Intialize random forest regressor

random_forest_regressor.fit(X_train, y_train)

y_pred = random_forest_regressor.predict(X_test)# make prediction on testing data

"""**MODEL EVALUATION OF RANDOM FOREST MODEL**"""

# Evaluate the model using random forest
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print the evaluation metrics
print("R-squared :", r2)
print("Mean Squared Error (MSE):", mse)

"""OBSERVATIONS


*   Here,I created two models , one using linear regression algorithm and other using random forest algorithm.

*   In the linear regression(model1), R squared is 0.8743921699317792
                               Mean Squared Error: 9915987.159554115

*   In random forest(model2) ,R-squared : 0.9550905373589301
                            Mean Squared Error : 3545333.5564308297


From this metrics it is observed that,

*  Random forest model has high R squared as compared to linear regression which means the second model explains higher proportion of variance in target variable.

*  Also , in the case of MSE , model 2 has lower mse than model1 which indicates that model2 has lower error .


Therefore, in conclusion I can say that model2 perform better than model1. It demonstrates better performance.






"""